---
output:
  word_document: default
  html_document: default
---
# Load libraries

```{r}
rm(list=ls())

library(AER)
library(car)
library(FactoMineR)
```


# Load data and take a look

```{r}
file_path = "C:/Users/tomas/Downloads/rawData/heart.csv"
df = read.csv(file_path, header = T)
head(df)
```

# Categorical data: change numeric value to categorical

```{r}
df$sex[which(df$sex==0)] <- "Female"
df$sex[which(df$sex==1)] <- "Male"
df$sex <- as.factor(df$sex)

df$cp[which(df$cp==0)] <- "Typical Angina"
df$cp[which(df$cp==1)] <- "Atypical Angina"
df$cp[which(df$cp==2)] <- "Non-anginal Pain"
df$cp[which(df$cp==3)] <- "Asymptomatic"
df$cp <- as.factor(df$cp)

df$fbs[which(df$fbs==0)] <- "<= 120 mg/dL"
df$fbs[which(df$fbs==1)] <- "> 120 mg/dL"
df$fbs <- as.factor(df$fbs)

df$restecg[which(df$restecg==0)] <- "Normal"
df$restecg[which(df$restecg==1)] <- "Abnormality"
df$restecg[which(df$restecg==2)] <- "Hypertrophy"
df$restecg <- as.factor(df$restecg)

df$exang[which(df$exang==0)] <- "No"
df$exang[which(df$exang==1)] <- "Yes"
df$exang <- as.factor(df$exang)

df$slope[which(df$slope==0)] <- "Upsloping"
df$slope[which(df$slope==1)] <- "Flat"
df$slope[which(df$slope==2)] <- "Downsloping"
df$slope <- as.factor(df$slope)

df$thal[which(df$thal==0)] <- "Normal"
df$thal[which(df$thal==1)] <- "Fixed Defect"
df$thal[which(df$thal==2)] <- "Reversible Defect"
df$thal <- as.factor(df$thal)
```


```{r}
head(df)


hist(df$age)
plot(df$sex)

df$age <- as.numeric(df$age)
df$trestbps <- as.numeric(df$trestbps)
df$chol <- as.numeric(df$chol)
df$thalach <- as.numeric(df$thalach)
df$oldpeak <- as.numeric(df$oldpeak)

sapply(df, class)
```


# Detection of errors

```{r}
summary(df)

df$thal[which(df$thal==3)] <- NA
df$ca[which(df$ca==4)] <- NA

miss_val = sum(is.na(df))
miss_val
 
summary(df)
```

# Univariate detection

```{r}
outliers <- function(column, name){
  
  sumlist <- summary(column)
  q1 <- sumlist[2] 
  q3 <- sumlist[5]
  
  
  boxplot(column, main = paste("Boxplot ", name), col = "orange", horizontal = T)
  
  # IQR calculation
  iqr <- q3 - q1 
  
  # Mild inferior limit:
  mild_inf_lim <- sumlist[2]-1.5*iqr
  # Extreme inferior limit:
  extreme_inf_lim <- sumlist[2]-3*iqr
  abline(v=mild_inf_lim, col = "red", lty = 2)
  abline(v=extreme_inf_lim, col = "red",  lty = 2, lwd = 2)
  mild_sup_lim <- sumlist[5]+1.5*iqr
  extreme_sup_lim <- sumlist[5]+3*iqr
  abline(v=mild_sup_lim, col = "red", lty = 2)
  abline(v=extreme_sup_lim, col = "red",  lty = 2, lwd = 2)
  
}
```

```{r}
outliers(df$age, "age")
outliers(df$trestbps, "trestbps")
outliers(df$chol, "chol")
outliers(df$thalach, "thalach")
outliers(df$oldpeak, "oldpeak")
outliers(df$ca, "ca")
outliers(df$target, "target")
```

```{r}
FindMildOutliers <- function(data) {
  lowerq = quantile(data, na.rm = TRUE)[2]
  upperq = quantile(data, na.rm = TRUE)[4]
  iqr = upperq - lowerq #Or use IQR(data)
  # we identify extreme outliers
  extreme.threshold.upper = (iqr * 1.5) + upperq
  extreme.threshold.lower = lowerq - (iqr * 1.5)
  result <- which(data > extreme.threshold.upper | data < extreme.threshold.lower)
}

FindExtremeOutliers <- function(data) {
  lowerq = quantile(data, na.rm = TRUE)[2]
  upperq = quantile(data, na.rm = TRUE)[4]
  iqr = upperq - lowerq #Or use IQR(data)
  # we identify extreme outliers
  extreme.threshold.upper = (iqr * 3) + upperq
  extreme.threshold.lower = lowerq - (iqr * 3)
  result <- which(data > extreme.threshold.upper | data < extreme.threshold.lower)
}


FindMissingValues <- function(data) {
  result <- which(sum(is.na(data)) > 0)
}

```

## Finding all Missing Values by Variables
```{r}
miss <- colSums(is.na(df))
rank_miss <- sort(miss, decreasing = TRUE)
rank_miss
```

## Finding all Extreme Outliers by Variables
```{r}
df2 <- Filter(is.numeric, df)
pos <- lapply(df2, FindExtremeOutliers)
extr_out <- lengths(pos)
num_outliers = length(pos)
rank_extr <- sort(extr_out, decreasing = TRUE)
rank_extr
```


## Finding all Mild Outliers by Variables
```{r}
df2 <- Filter(is.numeric, df)
pos <- lapply(df2, FindMildOutliers)
mild_out <- lengths(pos)
rank_mild <- sort(mild_out, decreasing = TRUE)
rank_mild
```

## Finding all Missing Values by Individuals
```{r}
pos <- apply(df, 1, FindMissingValues)
pos <- which(pos > 0)
pos
```

## Finding all Extreme Outliers by Individuals
```{r}
df2 <- Filter(is.numeric, df)
posExtremeInd <- apply(df2, 1, FindExtremeOutliers)
posExtremeInd <- which(posExtremeInd > 0)
posExtremeInd
```

## Finding all Mild Outliers by Individuals
```{r}
df2 <- Filter(is.numeric, df)
pos <- apply(df2, 1, FindMildOutliers)
pos <- which(pos > 0)
pos
length(pos)
```


## Create variable adding the total number missing values, outliers and errors.

```{r}
num_outliers_miss_errors = miss_val + num_outliers
num_outliers_miss_errors
```
# Imputation

## Imputation of factor thal
```{r}
library(missMDA)
# Categorical imputation 
f <- Filter(is.factor, df)
vars_dis = colnames(f)
summary(df[,vars_dis])
res.input<-imputeMCA(df[,vars_dis],method="EM")
summary(res.input$completeObs)


# Validation
barplot(table(df$thal),col="red")
barplot(table(res.input$completeObs[,1]),col="blue")

df[,vars_dis] <- res.input$completeObs
summary(df)

miss_val = sum(is.na(df))
miss_val

```




## Imputation of numeric ca

```{r}
library(class)
# Numeric imputation  only explanatory variables - never for target
n <- Filter(is.numeric, df)
summary(n)
vars_con = colnames(n)

fullvariables <- c(1,4,5,8,10,14)
aux <- df[,fullvariables]
dim(aux)
names(aux)

summary(df)

aux1 <- aux[!is.na(df$ca),]
dim(aux1)



aux2 <- aux[is.na(df$ca),]
dim(aux2)

knn.ing = knn(aux1, aux2, df$ca[!is.na(df$ca)])

df$ca[is.na(df$ca)] <- as.numeric(as.character(knn.ing))

# Validation
summary(df)


miss_val = sum(is.na(df))
miss_val
```


## Compute the correlation with all other variables. Rank these variables according the correlation
```{r}
library(FactoMineR)
library(mvoutlier)
df2 <- Filter(is.numeric, df)
res <- cor(df2)
round(res, 2)
```

```{r}
library(corrplot)
corrplot(res)
```

## Correlation variables and all other variables
```{r}
condes(df, 1)
# age & sex -> There is a almosts nonexistent dependance between these two variables, since p-value < 0.05
# age & cp -> There is a almosts nonexistent dependance between these two variables, since p-value < 0.05
# age & trestbps -> There is slight positive correlation between the age and trestbps,since p-value < 0.05
# age & chol -> There is slight positive correlation between the age and chol,since p-value < 0.05
# age & fbs -> There is a slight dependance between these two variables, since p-value < 0.05
# age & restecg -> There is a almosts nonexistent dependance between these two variables, since p-value < 0.05
# age & thalach -> There is a near moderate negative correlation between these variables, since p-value < 0.05
# age & exang -> Exang has no effect on the va  lues of age, p-value > 0.05
# age & oldpeak -> There is slight positive correlation between the age and oldpeak ,since p-value < 0.05
# age & slope -> There is a almosts nonexistent dependance between these two variables, since p-value < 0.05
# age & ca -> There is slight positive correlation between the age and ca,since p-value < 0.05
# age & thal ->  There is a almosts nonexistent dependance between these two variables, since p-value < 0.05
# age & target -> There is a slight negative correlation between these variables, since p-value < 0.05


#Correlation with sex and all other variables
#catdes(df, 2)
# sex & age -> Age has a no effect in the value of sex, since p-value > 0,05
# sex & cp -> 34.93% of Female have Non-anginal Pain, 42.63% of Female have Typical Angina,
#             and 4.17% are Asymptomatic.
#             8.98% of Male are Asymptomatic, 51.05% of Male have Typical Angina, and 24.54%
#             of Male have Non-anginal Pain.
# sex & trestbps -> Trestbps has no effect in the value of sex, since p-value > 0,05
# sex & chol -> Chol has a small to medium effect in the value of sex, since p-value < 0,05
# sex & fbs -> Fbs is not significant.
# sex & restecg -> 3.53% of Female have Hypertrophy, and the rest are not significant.
#                  0.56% of Male have Hypertrophy, and the rest are not significant.
# sex & thalac -> Thalac is not significant.
# sex & exang -> 76.28% of Female have the value of NO, and the other 23.72% have a value of YES.
#                38.01% of Male have a value of YES, and the other 61.99% have a value of NO.
# sex & oldpeak -> Oldpeak has no effect in the value of sex, since p-value > 0,05
# sex & slope -> Slope is not significant.
# sex & ca -> Ca has a small effect in the value of sex, since p-value < 0,05
# sex & thal -> 80% of Female have Reversible Defect, 1.28% of Female have Fixed Defect
#               and 16.99% have no value.
#               50.07% of Male have no value, 8.42% of Male have Fixed Defect, 40.95% of
#               Male have Reversible Defect.
# sex & target -> Target has a medium effect in the value of sex, since p-value < 0,05


# Correlation with cp and all other variable
#catdes(df, 3)
#cp & age -> Age has a small effect in the value of cp, since p-value < 0.05
#cp & sex -> 83.11% of Asymptomatic are Male and the other 16.88% are Female. 
#            Sex has no effect on the people who has Atypical Angina.
#            61.62% of Non-anginal Pain are Male and the other 38.38% are Female.
#            73.24% of Typical Angina are Male and the other 26.76% are Female.
#cp & trestbps -> Trestbps has a medium effect in the value of cp, since p-value < 0.05.
#cp & chol -> Chol has no effect in cp's value, since p-value > 0.05.
#cp & fbs -> Fbs has no effect on the people who is Asymtomatic.
#            90.4% of Atypical Angina has fbs<=120 mg/dL and the other 9.58% has fbs>120 mg/dL.
#            19.36% of Non-anginal Pain has fbs > 120 mg/dL and the other 80.63% has fbs<=120 mg/dL.
#            Fbs has no effect on the people who has Typical Angina.
#cp & restecg -> restecg has no effect on the people who is Asymtomatic.
#               61.67% of Atypical Angina has abnormal restecg, 38.32% has normal restecg and the remaining Hypertrophy restecg has no effect.
#               41.19% of Non-anginal Pain has normal restecg,  57.74% has abnormal restecg and the remaining Hypertrophy restecg has no effect.
#               54.32% of Typical Angina has normal restecg, 43.26% has abnormal restecg and the remaining Hypertrophy restecg has no effect.
#cp & thalach -> Thalach has a large effect in the value of cp, since p-value < 0.05
#cp & exang -> 83.11% of Asymptomatic has no exang and the other 16.88% has exang.
#             92.81% of Atypical Angina has no exang and the other 7.18% has exang.
#             86.97% of Non-anginal Pain has no exang and the other 13.02% has exang.
#             56.94% of Typical Angina has exang and the other 43.06% has no exang.
#cp & oldpeak -> Oldpeak has a large effect in the value of cp, since p-value < 0.05
#cp & slope -> slope has no effect on the people who is Asymtomatic.
#             70.66% of Atypical Angina has downsloping slope, 24.55% has flat slope and the remaining Upsloping slope has no effect.
#             55.98% of Non-anginal Pain has downsloping slope, 38.73% has flat slope and the remaining Upsloping slope has no effect.
#             58.75% of Typical Angina has flat slope, 32.79% has downsloping slope and the remaining Upsloping slope has no effect.
#cp & ca -> Ca has a medium effect in the value of cp, since p-value < 0.05
#cp & thal -> thal has no effect on the people who is Asymtomatic.
#             thal has no effect on the people who has Atypical Angina.
#             95.07% of Non-anginal Pain has reversible defect thal, 3.87% has fixed defect and the remaining normal thal has no effect.
#             85.51% of Typical Angina has reversible defect thal, 13.68% has fixed defect and the remaining normal thal has no effect.
#cp & target -> Target has a large effect in the value of cp, since p-value < 0.05

#condes(df, 4)
# trestbps & age -> There is slight positive correlation between trestbps and age, since p-value < 0.05
# trestbps & sex -> There is an almost nonexistent dependence between these two variables, since p-value < 0.05
# trestbps & cp -> There is an almost nonexistent dependence between these two variables, since p-value < 0.05
# trestbps & chol -> There is slight positive correlation between the trestbps and chol,since p-value < 0.05
# trestbps & fbs -> There is a slight dependence between these two variables, since p-value < 0.05
# trestbps & restecg -> There is an almost nonexistent dependence between these two variables, since p-value < 0.05
# trestbps & thalach -> Talach has no effect on the values of trestbps, p-value > 0.05
# trestbps & exang -> Exang has no effect on the values of trestbps, p-value > 0.05
# trestbps & oldpeak -> There is slight positive correlation between trestbps and oldpeak ,since p-value < 0.05
# trestbps & slope -> There is an almost nonexistent dependence between these two variables, since p-value < 0.05
# trestbps & ca -> There is slight positive correlation between trestbps and ca,since p-value < 0.05
# trestbps & thal ->  There is an almost nonexistent dependence between these two variables, since p-value < 0.05
# trestbps & target -> There is a slight negative correlation between these variables, since p-value < 0.05

# Correlation with chol and all other variable
#condes(df, 5)
#chol & age -> There is a slight positive correlation between the chol and age,since p-value < 0.05
#chol & sex -> There is a almost nonexistent dependence between these two variables, since p-value < 0.05
#chol & cp -> Only the category Typical Angina has a positive (or significant) impact in the mean of chol
#chol & trestbps -> There is a non to a slight positive correlation between these variables, since p-value < 0.05
#chol & fbs -> Fbs is not significant
#chol & restecg -> There is a almost nonexistent dependence between these two variables, since p-value < 0.05
#chol & thalach -> Thalach is not significant
#chol & exang -> There is a almost nonexistent dependence between these two variables, since p-value < 0.05
#chol & oldpeak -> There is a almost nonexistent correlation between these two variables, since p-value < 0.05
#chol & slope -> Only the category Flat has a positive (or significant) impact in the mean of chol
#chol & ca -> There is a almost nonexistent correlation between these two variables, since p-value < 0.05
#chol & thal -> There is a almost nonexistent dependence between these two variables, since p-value < 0.05
#chol & target -> There is a non to slight negative correlation between these two variables, since p-value < 0.05


# Correlation with fbs and all other variable
#catdes(df, 6)
#fbs & age -> Age has a small effect in the value of fbs, since p-value < 0.05.
#fbs & sex -> Sex has no effect on people who has fbs <= 120 mg/dL neither on fbs > 120 mg/dL.
#fbs & cp -> 17.32% of people who has fbs <= 120 mg/dL has Atypical Angina cp, the 26.26% has Non-anginal Pain cp, and the people with Typical Angina and Asymptomatic have no effect on fbs <= 120 mg/dL.
#           35.94% of people who has fbs > 120 mg/dL has Non-anginal Pain cp, the 10.45% has Atypical Angina cp, and the people with Typical Angina and Asymptomatic have no effect on fbs > 120 mg/dL.
#fbs & trestbps -> Trestbps has a small effect in the value of fbs, since p-value < 0.05.
#fbs & chol -> Chol has no effect in values of fbs, since p-value > 0.05.
#fbs & restecg -> 51.83% of people who has fbs <= 120 mg/dL has Abnormality restecg, the 46.44% has Normal restecg and the remaining Hypertrophy restecg has no effect on fbs <= 120 mg/dL.
#                 60.13% of people who has fbs > 120 mg/dL has Normal restecg, the 39.87% has Abnormality restecg and the remaining Hypertrophy restecg has no effect on fbs > 120 mg/dL.
#fbs & thalach -> Thalach has no effect in values of fbs, since p-value > 0.05.
#fbs & exang -> Exang has no effect in values of fbs, since p-value > 0.05.
#fbs & oldpeak -> Oldpeak has no effect in values of fbs, since p-value > 0.05.
#fbs & slope -> 6.07% of people who has fbs <= 120 mg/dL has Upsloping slope and the remaining Downsloping slope has no effect on fbs <= 120 mg/dL neither Flat slope.
#               13.72% of people who has fbs > 120 mg/dL has Upsloping slope and the remaining Downsloping slope has no effect on fbs > 120 mg/dL neither Flat slope.
#fbs & ca -> Ca has a small effect in the value of fbs, since p-value < 0.05.
#fbs & thal -> 91.97% of people with fbs >= 120 mg/dL has Reversible Defect thal, the 0.34% has Normal thal and the remaining 7.68% has Fixed Defect thal.
#             21.57% of people with fbs >= 120 mg/dL has Fixed Defect thal, the 2.61% has Normal thal and the remaining 75.82% has Reversible Defect thal.
#fbs & target -> Target has no effect in values of fbs, since p-value > 0.05.

#catdes(df, 7)
#restecg & age ->  Age has a small effect in the value of restecg, since p-value < 0.05.
#restecg & sex -> Sex has no effect on the people who have Abnormality.
#            73.33% of people with Hypertrophy are Female and the other 26.66% are Male.
#            Sex has no effect on the people considered "Normal".
#restecg & trestbps -> Trestbps has a small effect in the value of restecg, since p-value < 0.05.
#restecg & chol -> Chol has a small effect in restecg value, since p-value > 0.05.
#restecg & fbs -> 88.11% of people with Abnormality has fbs<=120 mg/dL and the other 11.89% has fbs>120 mg/dL
#            Fbs has no effect on people who have Hypertrophy.
#            81.48% of people considered "Normal" has fbs<=120 mg/dL and the other 18.52% has fbs>120 mg/dL
#restecg & thalach -> Thalach has a small effect in the value of restecg, since p-value < 0.05.
#restecg & exang -> 70.76% of people with Abnormality have Exang=No and the other 29.24% Exang=Yes.
#                   Exang has no effect on people who have Hypertrophy.
#                   62.37 of people considered "Normal" have Exang=No and the other 37.62% Exang=Yes.
#restecg & oldpeak -> Oldpeak has a small effect in the value of restecg, since p-value < 0.05
#restecg & slope -> 53.60% of people with Abnormality have Downsloping and the other 40.74% Flat slope.
#                   26.66% of people with Hypertrophy have Upsloping and the other 73.33% Flat slope.
#                   39.03% of people considered "Normal" have Downsloping and 52.71% Flat slope.
#restecg & ca -> Ca has a small effect in the value of restecg, since p-value < 0.05
#restecg & thal -> 7.79% of people with Abnormality have thal=Fixed Defect and the rest has no effect with Abnormality values.
#                  thal has no effect on people who have Hyperthrophy
#                  thal has no effect on people considered "normal"
#restecg & target -> Target has a small effect in the value of restecg, since p-value < 0.05


# Correlation with thalac and all other variable
#condes(df, 8)
#thalac & age -> There is a slight negative correlation between the thalac and age,since p-value < 0.05
#thalac & sex -> Sex is not significant
#thalac & cp -> There is a almost nonexistent dependence between these two variables, since p-value < 0.05
#thalac & trestbps -> Trestbps is not significant
#thalac & chol -> Thalach is not significant
#thalac & fbs -> Fbs is not significant
#thalac & restecg -> There is a almost nonexistent dependence between these two variables, since p-value < 0.05
#thalac & exang -> There is a almost nonexistent dependence between these two variables, since p-value < 0.05
#thalac & oldpeak -> There is a slight negative correlation between these two variables, since p-value < 0.05
#thalac & slope -> There is a almost nonexistent dependence between these two variables, since p-value < 0.05
#thalac & ca -> There is a slight negative correlation between these two variables, since p-value < 0.05
#thalac & thal -> There is a almost nonexistent dependence between these two variables, since p-value < 0.05
#thalac & target -> There is a non to slight negative correlation between these two variables, since p-value < 0.05


# Correlation with exang and all other variable
#catdes(df, 9)
# exang & age -> Age has non to small effect in the value of exang, since p-value > 0,05
# exang & sex -> Exang=NO:35.00% are Female, and the other 65.00% are Male.
#                Exang=YES: 78.55% are Male, and the other 21.45% are Female.
# exang & cp -> Exang=NO: 36.32% have Non-anginal Pain, 22.79% have Atypical Angina, 9.41% are Asymptomatic, and
#               31.47% have Typical Angina.
#               Exang=YES: 82.03% have Typical Angina, 3.77% are Asymptomatic, 3.48% have Atypical Angina, and 
#               10.72% have Non-anginal Pain
# exang & trestbps -> Trestbps has no effect in the value of exang, since p-value > 0,05
# exang & chol -> Chol has non to small effect in the value of exang, since p-value < 0,05
# exang & fbs -> Fbs is not significant.
# exang & restecg -> Exang=NO:53.38% have Abnormality, 45.59% are Normal and the rest is not significant.
#                    Exang=YES:54.2% are Normal, 43.48% have ABnormality and the rest is not significant.
# exang & thalac -> Thalac has a large effect in the value of exang, since p-value < 0,05
# exang & oldpeak -> Oldpeak has medium to large effect in the value of exang, since p-value > 0,05
# exang & slope -> Exang=NO: 56.18% have Downsloping, 37.94% have Flat, and 5.88% have Upsloping
#                  Exang=YES: 64.93% have Flat, 9.9% have Upsloping, and 25.22% have Downsloping 
# exang & ca -> Ca has a small effect in the value of exang, since p-value < 0,05
# exang & thal -> Exang=NO:92.5% have Reversible Defect, 7.05% have Fixed Defect
#                 and the rest is not significant.
#                 Exang=YES: 15.07% have Fixed Defect, 83.77% have Reversible Defect and the rest is not significant
# exang & target -> Target has a large effect in the value of exang, since p-value < 0,05


# Correlation with oldpeak and all other variables
#condes(df, 10)
# oldpeak & age -> There is a slight positive correlation between the oldpeak and age ,since p-value < 0.05
# oldpeak & sex -> There is almost nonexistent dependence between these two variables, since p-value < 0.05
# oldpeak & cp -> There is a medium dependence between these two variables, since p-value < 0.05
# oldpeak & trestbps -> There is slight positive correlation between the oldpeak and trestbps,since p-value < 0.05
# oldpeak & chol -> There is slight positive correlation between the oldpeak and chol,since p-value < 0.05
# oldpeak & fbs -> fbs has no effect on the values of oldpeak, since p-value > 0.05
# oldpeak & restecg -> There is a small dependence between these two variables, since p-value < 0.05
# oldpeak & thalach -> There is a near moderate negative correlation between these variables, since p-value < 0.05
# oldpeak & exang -> There is a medium dependence between these two variables, since p-value < 0.05
# oldpeak & slope -> There is a large dependence between these two variables, since p-value < 0.05
# oldpeak & ca -> There is a slight positive correlation between the oldpeak and ca,since p-value < 0.05
# oldpeak & thal ->  There is a medium dependence between these two variables, since p-value < 0.05
# oldpeak & target -> There is a near moderate negative correlation between these variables, since p-value < 0.05


# Correlation with slope and all other variable
#catdes(df, 11)
#slope & age -> Age has a small effect in values of slope, since p-value < 0.05 and eta2 < 0.06 and eta2 > 0.01.
#slope & sex -> Sex has no effect in values of slope, since p-value > 0.05.
#slope & cp -> 25.16% of Downsloping slope has Atypical Angina cp, 33.90% has Non-anginal Pain, 34.75% has Typical Angina and the remaining Asymptomatic values have no effect in Downsloping slope.
#             8.51% of Flat slope has Atypical Angina cp, 22.82% has Non-anginal Pain, 60.58% has Typical Angina and the remaining Asymptomatic values have no effect in Flat slope.
#             Cp has no effect in values of Upsloping slope.
#slope & trestbps -> Trestbps has a small effect in values of slope, since p-value < 0.05 and eta2 < 0.06.
#slope & chol -> Chol has no effect in values of slope
#slope & fbs -> fbs has no effect in values of Downsloping slope.
#               fbs has no effect in values of Flat slope.
#               28.38% of Upsloping slope values have fbs > 120 mg/dL and the remaining 71.62% have fbs <= 120 mg/dL.
#slope & restecg -> 58.64% of Downsloping slope values have Abnormality restecg, 41.36% have Normal restecg and 0% have Hypertrophy restecg.
#                   54.36% of Flat slope values have Normal restecg, 43.36% have Abnormality restecg and 2.28% have Hypertrophy restecg.
#                   5.40% of Uplsoping values have Hypertrophy restecg, and Normal restecg neither Abnormality restecg have effect in values of Upsloping slope.
#slope & thalach -> thalach has a large effect in values of slope, since p-value < 0.05 and eta2 > 0.14.
#slope & exang -> 81.45% of Downsloping slope values have no exang and 18.55% have exang.
#                 53.53% of Flat slope values have no exang and 46.47% have exang.
#                 54.05% of Upsloping slope values have no exang and 45.94% have exang.
#slope & oldpeak -> oldpeak has a large effect in values of slope, since p-value < 0.05 and eta2 > 0.14.
#slope & ca -> Ca has a small effect in values of slope, since p-value < 0.05 and eta2 < 0.06.
#slope & thal -> 97.87% of Downsloping slope values have Reversible Defect thal, 1.49% have Fixed Defect thal and Normal thal has no effect in Downsloping.
#               Thal has no effect in Flat slope values.
#               36.48% of Upsloping slope values have Reversible Defect thal, 63.51% have Fixed Defect thal and Normal thal has no effect in Upsloping.
#slope & target -> Target has a medium effect in values of slope, since p-value < 0.05 and eta2 < 0.14 and eta2 > 0.06.


# Correlation with ca and all other variables
#condes(df, 12)
# ca & age -> There is a slight positive correlation between the ca and age ,since p-value < 0.05
# ca & sex -> There is a small dependence between these two variables, since p-value < 0.05
# ca & cp -> There is a small dependence between these two variables, since p-value < 0.05
# ca & trestbps -> There is slight positive correlation between the ca and trestbps,since p-value < 0.05
# ca & chol -> There is slight positive correlation between the ca and chol,since p-value < 0.05
# ca & fbs -> There is a small dependence between these two variables, since p-value < 0.05
# ca & restecg -> There is a small dependence between these two variables, since p-value < 0.05
# ca & thalach -> There is a slight negative correlation between these variables, since p-value < 0.05
# ca & exang -> There is a small dependence between these two variables, since p-value < 0.05
# ca & oldpeak -> There is a slight positive correlation between the ca and oldpeak,since p-value < 0.05
# ca & slope ->There is a small dependence between these two variables, since p-value < 0.05
# ca & thal ->  There is a small dependence between these two variables, since p-value < 0.05
# ca & target -> There is a near moderate negative correlation correlation between these variables, since p-value < 0.05

#catdes(df, 13)
#thal & age ->  Age has no effect in the value of thal, since p-value > 0.05.
#thal & sex -> 89% of people with thal=Fixed Defect are Male and the other 11% are Female.
#              Sex has no effect on the people with thal=Normal.
#              32.46% of people with thal=Reversible Defect are Female and the other 67.54% are Male.
#thal & trestbps -> Trestbps has a small effect in the value of thal, since p-value < 0.05.
#thal & chol -> Chol has a small effect in thal value, since p-value > 0.05.
#thal & fbs -> 33% of people with thal=Fixed Defect has fbs>=120 mg/dL and the other 67% has fbs>120 mg/dL
#              57.14% of people with thal=Normal has fbs>=120 mg/dL and the other 42.85% has fbs>120 mg/dL
#              12.63% of people with thal=Reversible Defect has fbs<=120 mg/dL and the other 87.36% has fbs>120 mg/dL
#thal & thalach -> Thalach has a small effect in the value of thal, since p-value < 0.05.
#thal & exang -> 48% of people with thal=fixed defect have Exang=No and the other 52% Exang=Yes.
#                Exang has no effect on people with thal=Normal
#                68.51% of people with thal=Reversible Defect have Exang=No and the other 31.48% Exang=Yes.
#thal & oldpeak -> Oldpeak has a medium effect in the value of thal, since p-value < 0.05
#thal & slope -> 7% of people with thal=Fixed Defect have Downsloping and the other 47% Flat slope.
#                slope has no effect on people with thal=Normal
#                50% of people with thal=Reversible Defect have Downsloping and 2.94% Flat slope.
#thal & ca -> ca has no effect in the value of thal, since p-value > 0.05
#thal & restecg -> 40% of people with thal=Fixed Defect have Abnormality
#                  And the rest has no effect with thal values.
#thal & target -> Target has a small effect in the value of thal, since p-value < 0.05

```



## Heat Map correlacions
```{r}
df2 <- Filter(is.numeric, df)
df2 <- scale(df2)
cormat <- round(cor(df2),2)
library(reshape2)
melted_cormat <- melt(cormat)

library(ggplot2)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()
```

# Profiling

```{r}
# Continuous output:
library(FactoMineR)
summary(df$target)
res.condes <- condes(df, 14, proba = 0.50)
res.condes$quanti  # Global association to numeric variables
res.condes$quali # Partial association of numeric variables to levels of outcome factor
res.condes$category  # Partial association to significative levels in factors
```

# Multivariate outliers

```{r}
library(chemometrics)

summary(df[,vars_con])
mout<-Moutlier(df[,vars_con[1:5]],quantile = 0.9975, plot = TRUE)
# Classical: Assumption of normality on the underlying generating mechanism
# Robust: Median and absolute median deviations -> Not normal generating mechanism

length(which(mout$rd>mout$cutoff))
ll<-which(mout$rd>mout$cutoff)
Boxplot(mout$rd)
df[ll,c(vars_con)]
df$mout <- 0
df$mout[ ll ]<-1
df$mout <- factor( df$mout, labels=c( "NoMOut","YesMOut")) #We identify the Mildoutliers if the row has the value "YesMOut" in the mout column.
table(df$mout)
```


# Principal Components Analysis (PCA)
```{r}
#names(df)
vars_con
vars_dis
vars_res = c("target")


c(vars_res, vars_dis,vars_con, "mout") #All variables
#summary( df[ , c(vars_res, vars_dis,vars_con, "mout") ] )
res.pca<-PCA(df[,c(vars_res, vars_dis, vars_con)], quali.sup=c(2:8), quanti.sup= c(1,15)) 

# Multivariant outliers should be included as supplementary observations
ll <- which( df$mout == "YesMOut")
res.pca<-PCA(df[,c(vars_res, vars_dis, vars_con)],quali.sup=c(2:8),quanti.sup= c(1,15), ind.sup = ll ) 

#plot.PCA(res.pca,choix=c("var"),axes = c(1, 2))
#plot.PCA(res.pca,choix=c("var"),invisible=c("var"))
#plot.PCA(res.pca,choix=c("var"),invisible=c("quanti.sup","var"))
plot.PCA(res.pca,choix=c("ind"),invisible=c("ind"))
```
Aquest extrany PCA pot ser resultat de que la mostra que s'ha utilitzat per estudiar es gent jove amb problemes de cor principalment i en canvi la gent d'una major edat que ha format part de l'estudi no té aquests problemes. Això fa que quan més jove, doncs més probable de que tinguis problemes de cor. Però el colesterol, si que està correlacionat positivament en l'eix de les Y, que representa el conjunt de variables relacionades amb la sang. 
Ara bé, el "ca" o "oldpeak", són també causes d'atac de cor, al ser problemes comuns en gent gran, es correlacionen negativament amb el target pel que s'ha esmentat anteriorment.



```{r}
par(mfrow = c(1, 2))

plot(df[,1], df[,14]) #Here we can verify that there is only people with problems or people with good heart health.
hist(df[,1]) #Here we verify that the most part of our population has around 60 years old, but the majority of those individuals does not have heart problems. And the most part of our population that has problems is under those 50 years old. In addition, the youngest inviduals, younger than 35, the majority has problems.

plot(df[,12], df[,14])

plot(df[,10], df[,14])
plot(df[,1], df[,10]) #Here we conclude that not matter how bad a high oldpeak is, that in our individuals with heart problems they have it correct. So in reality is a factor that the bigger, the more probability to have a problem. But for our individuals in this study we cannot conclude that. And this affects the PCA.

#PCA with at least 45 years 
df2 <- df[df$age < 45,]

res.pca45<-PCA(df2[,c(vars_res, vars_dis, vars_con)], quali.sup=c(2:8), quanti.sup= c(1,15)) 

# Multivariant outliers should be included as supplementary observations
ll <- which( df2$mout == "YesMOut")
res.pca45<-PCA(df2[,c(vars_res, vars_dis, vars_con)],quali.sup=c(2:8),quanti.sup= c(1,15), ind.sup = ll ) 


```

Eigenvalues and dominant axes analysis. How many axes we have to interpret according to Kaiser and Elbow's rule?

Individuals point of view: Are they any individuals "too contributive"? To better understand the axes meaning use the extreme individuals. Detection of multivariate outliers and influent data.

Interpreting the axes:  Variables point of view coordinates, quality of representation, contribution of the variables  
Perform a PCA taking into account also supplementary variables the supplementary variables can be quantitative and/or categorical 

```{r}
res.pca$eig
```
According to the Kaiser rule we have to take into account those components which have the 80% of the whole variables. So in this case we should consider comp 1 (35.38%), comp 2 (19.65%), comp 3 (14.99%) and comp 4 (11.71%)


```{r}
res.pca$eig[,1]

# Eigenvalues are the square of the singular values (sdev)
eigenvalues <- res.pca$eig[,1]

# Calculate the proportion of variance explained
total_variance <- sum(eigenvalues)
explained_variance <- eigenvalues / total_variance

print(explained_variance)

summary(res.pca, nb.dec = 2, ncp = 3, nbelements = 1)

par(mfrow = c(1, 2))
barplot(res.pca$eig[, 1], main = "Eigenvalues", names.arg = paste("dim", 1:nrow(res.pca$eig)))

plot(explained_variance, xlab = "Principal Component", ylab = "Proportion of Variance Explained", type = 'b', main = "Scree Plot")

```

Here we can confirm that we need the first 4 components to at least represent the 80% of the variables. We also have added a barplot were we also can see that the diference in the values represented between 4th and 5th are minimum.


Using the res.pca45, the PCA done by age greater than 45 years.

```{r}
res.pca45$eig
```

According to the Kaiser rule we have to take into account those components which have the 80% of the whole variables. So in this case we should consider comp 1 (26.35%), comp 2 (22.96%), comp 3 (17.87%) and comp 4 (13.8%)


```{r}
res.pca45$eig[,1]

# Eigenvalues are the square of the singular values (sdev)
eigenvalues45 <- res.pca45$eig[,1]

# Calculate the proportion of variance explained
total_variance45 <- sum(eigenvalues45)
explained_variance45 <- eigenvalues45 / total_variance45

print(explained_variance45)

summary(res.pca45, nb.dec = 2, ncp = 3, nbelements = 1)

par(mfrow = c(1, 2))
barplot(res.pca45$eig[, 1], main = "Eigenvalues", names.arg = paste("dim", 1:nrow(res.pca45$eig)))

plot(explained_variance, xlab = "Principal Component", ylab = "Proportion of Variance Explained", type = 'b', main = "Scree Plot")

```

Here we can confirm that we need the first 4 components to at least represent the 80% of the variables. We also have added a barplot were we also can see that the diference in the values represented between 4th and 5th are minimum.

## Individuals point of view

### Contribution

```{r}
library(factoextra)
fviz_pca_ind(res.pca, col.ind="contrib", geom = "point") +
scale_color_gradient2(low="darkslateblue", mid='white', high='red', midpoint=0.40)
```
We can see that there are only two individuals who are slightly more contributive. There are rather more individuals who have a low contribution. 

### Extreme individuals 

#### In dimension 1

We now look for the extreme individuals to better understand the axes meaning. First we plot them.
```{r}
rang<-order(res.pca$ind$coord[,1])

contrib.extremes<-c(row.names(df)[rang[1:10]], row.names(df)
[rang[(length(rang)-10):length(rang)]])
fviz_pca_ind(res.pca, select.ind = list(names=contrib.extremes))
```
Here we have a closer look at the extrem individuals.
```{r}
df[which(row.names(df) %in% row.names(df)[rang[length(rang)]]), 1:15]

df[which(row.names(df) %in% row.names(df)[rang[1]]),1:15]
```
#### In dimension 2

We replicate the procedure conducted for dimension one, now focusing on dimension two.

```{r}
rang<-order(res.pca$ind$coord[,2])

contrib.extremes<-c(row.names(df)[rang[1:10]], row.names(df)
[rang[(length(rang)-10):length(rang)]])
fviz_pca_ind(res.pca, select.ind = list(names=contrib.extremes))
```
```{r}
df[which(row.names(df) %in% row.names(df)[rang[length(rang)]]), 1:15]

df[which(row.names(df) %in% row.names(df)[rang[1]]),1:15]
```

## Interpreting the axes

### Variables point of view coordinates

Initially, we examine the contribution of each variable to the dimensions. Age appears as the most significant contributor to the first dimension, while cholesterol (chol) takes precedence in the second dimension. Notably, variables such as thalac, oldpeak, and ca only contribute significantly to one of the two dimensions.

```{r}
res.pca$var$coord
```

### Quality of representation


Here, we observe the degree of representation for each category across the first four dimensions. In the plane of the first and second dimensions, all categories are well represented. However, in the plane of the third and fourth dimensions, categories such as oldpeak, thalac, and age show comparatively lower levels of representation.

```{r}
res.pca$var$cos2
fviz_cos2(res.pca, choice = "var", axes = 1:2)
fviz_cos2(res.pca, choice = "var", axes = 3:4)
```

### Contribution of the variables


Here, we seek to understand the contribution of categories across planes of different dimensions. We observe a similar pattern to that seen in representation of the categories. 


```{r}
res.pca$var$contrib
fviz_contrib(res.pca, choice = "var", axes = 1:2)
fviz_contrib(res.pca, choice = "var", axes = 3:4)
```

# K-Means Clustering

```{r}
dim(res.pca$ind$coord[, 1:2])
#dist(res.pca$ind$coord[, 1:2])

res.pca<-PCA(df[,c(vars_res, vars_dis, vars_con)], quali.sup=c(2:8), quanti.sup= c(1,15)) 

kc <- kmeans(res.pca$ind$coord[, 1:2], centers = 4) 
```

Quality - Suma de quadrats total = Suma de quadrats intra + Suma de quadrats inter
```{r}
100*kc$betweenss/kc$totss 
```

```{r}
df$kmeaclu <- factor(kc$cluster)

res.catdes <- catdes(df, 16)

res.catdes$test.chi2
```

cp (p = 1.21e-42, df=9) and slope (p=3.62e-42, df=6) : A very small p-value suggests this variables have a significant association with the clusters. With 9 and 6 degrees of freedom, which suggests that the variables have several categories influencing how data is grouped into these clusters, since df > num. clusters.

exang (p=4.02e-26, df=3) and mout(p=6.29e-22, df=3): A significant p-value with fewer degrees of freedom than number of clusters, suggesting a strong association with the clusters.

restecg (p=1.08e-13, df=6) and thal (p=1.27e-07): Still a significant p-value, with 6 degrees of freedom.

sex (p=2.50e-07, df=3) and fbs (p=5.25e-06, df=3): Also significant p-value, since it is smaller than 0.05, with 3 degrees of freedom.

```{r}
res.catdes$category[1]
```

This first cluster has high influence in individuals with sex=Female, cp=Asympomatic and fbs>=120 mg/dL. The mean of this variables in this cluster is significantly higher than the global one.

As less characteristic variables we can see sex=Male, cp=Typical Angina and fbs<=120 mg/dL. The mean in the cluster is significantly lower than the global one.

```{r}
res.catdes$category[2]
```

The second cluster has high influence in individuals with cp=Typical Angina, slope=Flat and exang = Yes. We can see this is really contrary to the first cluster, the less characteristic variables are slope=Downsloping, cp=Non-anginal Pain and exang = No.

```{r}
res.catdes$category[3]
```

This cluster has high influence in individuals with sex=Female, cp=Asympomatic and fbs>=120 mg/dL. The mean of this variables in this cluster is significantly higher than the global one.

As less characteristic variables we can see sex=Male, cp=TypicalAngina and fbs<=120 mg/dL. The mean in the cluster is significantly lower than the global one.

```{r}
res.catdes$category[4]
```

Finally we have a cluster that has high influence in individuals with exang=Yes, cp=Typical Angina and slope = flat.

And less influence in individuals with sex=Female, cp=Atypical Angina and exang=No. 

We can observe contrary characteristics between clusters, which makes sense.

```{r}
res.catdes$quanti.var
```

The values which seem to help the most to differentiate between clusters are age, talach(maximum heart rate), oldpeak (ST depression induced by exercise) and ca, with this same order, both thalach and oldpeak related to cardiovascular characteristics and also age as an important factor.

On the other hand both chol and trestbps seem to be the continuous variables that help less in differentiating clusters. 

```{r}
res.catdes$quanti[1]
```

We can see this first cluster has a higher influence in individuals with a higher maximum heart rate, a lower cholesterol and younger in comparison to the overall mean. 

```{r}
res.catdes$quanti[2]
```

The indiviudals in this second cluster are older, have higher cholesterol and a really low maximum heart rate, they also have a higher ST depression caused by activity in comparison to rest. The characteristics of this individuals look contrary to the individuals in the first cluster.

```{r}
res.catdes$quanti[3]
```

Individuals in this third clusters have a really high cholesterol, a high resting blood pressure(trest bps), and tend to be older than the global mean. Their overall higher heart rate and lower oldpeak suggest a better exercise condition in comparision to cluster 2.

```{r}
res.catdes$quanti[4]
```

Finally, indivuals in the fourth cluster have a similar to the mean oldpeak, low max heart rate and low cholesterol. Age seems to not be relevant in individuals that ifluence this cluster.  

# Hierarchical Clustering (we will do it using the res.pca)

We continue now with hierarchical clustering. The function we will use is in the package FactoMineR. This function can work already with the result of the PCA function.

```{r}
res.pca<-PCA(df[,c(vars_res, vars_dis, vars_con)], quali.sup=c(2:8), quanti.sup= c(1,15)) 

res.hcpc <- HCPC(res.pca, nb.clust=-1)

```

```{r}
res.hcpc
```
Global View of the Results
```{r}
head(res.hcpc$data.clust$clust, 100)
table(res.hcpc$data.clust$clust)

res.hcpc$desc.var  
```

Test of the association of categorical variables
```{r}
res.hcpc$desc.var$test.chi2  
```
slope (p = 8.18e-42, df = 6): The extremely small p-value suggests that the variable 'slope' has a statistically significant association with the clusters. With 6 degrees of freedom, which suggests that the variable has several categories influencing how data is grouped into these clusters, since df > number of clusters.

cp (p= 4.36e-33, df = 9): Again, a very small p-value indicating strong statistical significance. With 9 degrees of freedom, which suggests that the variable has several categories influencing how data is grouped into these clusters. 

exang (p = 2.98e-23, df = 3): A significant p-value with fewer degrees of freedom (3), suggesting a strong association between 'exang' and the clusters.

restecg (p = 3.69e-14, df = 6): This also shows a significant relationship but with a slightly higher p-value compared to the first three variables, indicating a somewhat less strong association with the clusters.

sex (p = 5.42e-06, df = 3): While still statistically significant, 'sex' has the least strong association among the variables listed,since it has the lower p-value. And with only 3 degrees of freedom.

thal (p = 1.21e-05, df = 6): 'thal' shows a significant but less strong association compared to the top variables, but with more degrees of freedom (6) than some of them, which suggests that the variable has several categories influencing how data is grouped into these clusters.

fbs (p = 1.76e-05, df = 3): Significant p-value, since it is smaller than 0.05, but less so than the top variables. And with 2 degrees of freedom.

 




Tests of categorical association
```{r}
res.hcpc$desc.var$category[1]
```

In cluster 1:
We can detect several variables that have a strong influence in it. 
The first is the 'slope=Downsloping', since it has a very high and positive v-test value. And also we can see that the mean in the cluster is significantly higher than the global one. This same, occurs in 'exang=No' and 'res.tecg=restecg_Abnormality'.

And then the variable that are less characteristic, are the 'exang=Yes', 'slope=Flat' and 'cp=Typical Angina', in contrast of the variables with strong influence, have significant negative v.test values and the mean in the cluster is significantly lower than the global one, suggesting they are not typical of this cluster.


```{r}
res.hcpc$desc.var$category[2]
```

In cluster 2: 
Here we can see a gender influence, since 'sex=Female' has a high positive v-test and a mean in the cluster is significantly higher than the global one. And the 'sex=Male' is a high negative v-test and a mean in the cluster is significantly lower than the global one.

Also a good point to mention is that 'cp=Asymptomatic' and 'restecg=restecg_Normal' are characteristic of this cluster, with positive v.test values and with a mean in the cluster is significantly higher than the global one.

And as well as the gender, the blood sugar levels are also significant, 'fbs=<= 120 mg/dL' and 'fbs=> 120 mg/dL' with corresponding positive and negative influences.


```{r}
res.hcpc$desc.var$category[3]
```
In cluster 3:
Catgories like exang=Yes, slope=Flat, and cp=Typical Angina are significantly more characteristic of this cluster, indicating specific profiles or conditions more prevalent within this cluster compared to the overall dataset.
Since they have a mean in the cluster significantly higher than the global one. Also all of them have a very high positive v-test value suggesting, again that those categories are more prevalent in this cluster than globally.

Conversely, catgories like cp=Atypical Angina, slope=Downsloping, and exang=No are less characteristic of this cluster, occurring less frequently than expected globally.


```{r}
res.hcpc$desc.var$category[4]
```

In cluster 4:
In this fourth cluster cp=Typical Angina: Dominates the cluster, indicating that typical angina is a primary symptom for many within this group. This suggests a specific cardiac issue or pain profile that is prevalent. 

Slope Characteristics (Flat and Upsloping): Both flat and upsloping responses during exercise tests are significantly more common in this cluster compared to the global average, highlighting unique physiological responses to exercise among these patients.

Then, thal=thal_Fixed Defect: A higher prevalence of fixed defects in thallium stress tests characterizes this cluster, pointing to certain types of heart muscle damage or abnormalities. 

The Electrocardiographic Findings: The cluster shows a higher occurrence of normal and hypertrophic findings in resting ECGs, indicating specific electrocardiographic profiles.

Finally, Gender (Male): Males are more prevalent in this cluster, suggesting possible gender-related susceptibility or risk factors in cardiac conditions featured within the group.

On the side of the less characteristic categories, we can see the thal=thal_Reversible Defect, Restecg Abnormalities, Absence of Exercise-Induced Angina (exang=No), Chest Pain Types (Non-anginal Pain and Atypical Angina) and Slope=Downsloping.


Global tests of numerical association.
```{r}
res.hcpc$desc.var$quanti.var
```

To begin with, the thalach (Maximum heart rate achieved), has a very high eta2, suggesting that maximum heart rate significantly differentiates the clusters. This could imply that different clusters may be characterized by differing levels of physical fitness or cardiac function.

For the variable oldpeak, which represents the ST depression induced by exercise relative to rest, has as well as the thalach a high eta2 value, indicating a strong influence of exercise-induced ST depression on cluster differentiation, which might relate to underlying differences in cardiac ischemia among clusters.

The eta2 value for age is also high, suggesting significant variation in age across clusters, which could reflect differing disease prevalence or risk factors across age groups.

The eta2 value that we got for the Ca variable (Number of major vessels colored by fluoroscopy), it still being high indicating notable differences in the prevalence of observable coronary artery disease across clusters.

For the trestbps (Resting blood pressure) variable, we got a similar value as the Ca, which indicates a considerable influence of resting blood pressure on clustering, possibly reflecting different cardiovascular risk profiles.

And now,the following variable chol (Serum cholesterol), we find lower eta2 values than the others but still notable, suggesting variations in cholesterol levels influence cluster characteristics.


Specific tests of numerical association.
```{r}
res.hcpc$desc.var$quanti[1]
```

Cluster 1 is characterized by younger individuals with higher heart rates, a higher presence of the target condition, but lower blood pressure, cholesterol, ST depression, and fewer major vessels detected by fluoroscopy compared to the overall dataset. 

This profile could suggest a subgroup with distinct cardiovascular health characteristics, possibly at different stages of cardiovascular disease or with different risk factors compared to the average population in the study.


```{r}
res.hcpc$desc.var$quanti[2]
```

In the case of cluster 2 is characterized by individuals who are generally older with significantly higher cholesterol and resting blood pressure, suggesting a profile potentially indicative of higher cardiovascular risk. However, this group shows somewhat higher maximum heart rates and notably lower ST depressions during exercise, which could indicate varying degrees of cardiovascular fitness or response to exercise despite other risk factors. These attributes underscore the cluster's distinct cardiovascular health characteristics, which could be pivotal for targeted interventions or further study into risk management for these individuals.


```{r}
res.hcpc$desc.var$quanti[3]
```


For cluster 3 is characterized by older individuals with lower cholesterol, lower resting blood pressure, and lower maximum heart rates compared to the overall dataset. 

Additionally, there is a lower presence of the target condition (likely indicating a lower prevalence of a specific health outcome) within this cluster. These findings suggest that Cluster 3 may represent a subgroup with relatively better cardiovascular health or a different risk profile compared to other clusters in the dataset. 


```{r}
res.hcpc$desc.var$quanti[4]
```

Cluster 4 is characterized by individuals who tend to be older and exhibit significantly higher ST depressions during exercise, higher resting blood pressure, and a higher presence of major vessels colored by fluoroscopy compared to the overall dataset. However, despite these cardiovascular risk factors, individuals in this cluster demonstrate slightly higher cholesterol levels. 

Additionally, there is a lower presence of the target condition (likely indicating a lower prevalence of a specific health outcome) and lower maximum heart rates in this cluster. 

These findings suggest a subgroup with distinct cardiovascular health characteristics, possibly indicative of more advanced cardiovascular disease or different risk profiles compared to other clusters in the dataset. 

# Component Analysis (CA)


```{r}
library(FactoMineR)
library(car)
head(df)
summary(df)

dim(df)
dfc<-df[,c(1,4,5,8,10,12)]
con<-c(1,4,5,8,10,12)
row.sum <- apply(dfc,1, sum)
head(row.sum, 100)
# Column margins
col.sum <- apply(dfc,2, sum)
col.sum
# grand total
n <- sum(dfc)
n
```
```{r}
res.ca <- CA(dfc)
```
At first glipse, there seems to be some kind of dependence between thalac, tresbps, age and chol, due to the proximity of their values, also between ca and oldpeak.

```{r}
res.ca
summary(res.ca)
```
- Null hypothesis (H0): the row and the column variables of the contingency table are independent.
- Alternative hypothesis (H1): row and column variables are dependent
```{r}
chisq.test(dfc)
```
We see that pvalue is really small, then we can reject independence.
Let's find first the value for the innertia in our data:

```{r}
sum(res.ca$eig[,1])
```
We have an innertia of 0.0238 in our data.
Our maximum inertia in our dataset would be:
```{r}
max_inertia<-min(nrow(dfc)-1, ncol(dfc)-1)
max_inertia
```
How much of this inertia are we explaining?

```{r}
# How many components to choose?
res.ca$eig
```

To determine the number of eigenvalues different from 0, you can simply count the number of eigenvalues greater than 0 in the output you provided. In this case, there are 5 eigenvalues provided, and all of them are greater than 0.

This is indeed related to the number of dimensions or factors retained in the correspondence analysis. Each non-zero eigenvalue corresponds to a dimension that explains a certain amount of variance in the data.

In a contingency table used for correspondence analysis, the number of non-zero eigenvalues typically corresponds to the smaller of the number of rows minus one and the number of columns minus one. This is because the maximum number of dimensions that can be extracted is limited by the minimum of the number of rows and the number of columns in the contingency table
```{r}
eig.val <- res.ca$eig
barplot(eig.val[, 1], 
        names.arg = 1:nrow(eig.val), 
        main = "Innertia Explained by Dimensions (%)",
        xlab = "Principal Dimensions",
        ylab = "Percentage of innertia",
        col ="steelblue")
# Add connected line segments to the plot
lines(x = 1:nrow(eig.val), eig.val[, 1], 
      type = "b", pch = 19, col = "red")
```

```{r}
plot(res.ca, axes = 3:4)
```

```{r}
# row margins 
head(res.ca$call$marge.row, 20)
head(res.ca$call$marge.row*sum(dfc), 50)

# column margins 
res.ca$call$marge.col
res.ca$call$marge.col[rev(order(res.ca$call$marge.col))]*sum(dfc)
```


# Multiple Component Aanalysis (MCA)

Multiple Correspondence Analysis (MCA) it's an extension of Correspondence Analysis (CA), which can handle more than two categorical variables simultaneously. It helps to visualize patterns and associations between categories in a dataset by representing them in a lower-dimensional space.

We load the necessary libraries: 

```{r}
library(FactoMineR)
library(factoextra)
```

First, we compute MCA for our dataset, utilizing numerical variables as supplementary variables.

```{r}
res.mca <- MCA(df[,c(vars_res, vars_dis, vars_con)], quanti.sup = c(1,9:15), graph=FALSE)
```

When then look at the graph of the supplementary quantitative variables:

```{r}
fviz_mca_var(res.mca, choice="quanti.sup", repel=TRUE)
res.mca$quanti.sup
```
We observe that thalach and target exhibit stronger correlations with dimension 2, whereas the remaining quantitative variables show higher correlations with dimension 1. This trend becomes clearer when considering specific values. Notably, trestbps displays a similar level of correlation across both dimensions. Additionally, age appears to be more closely associated with the third dimension, while oldpeak relates more strongly to the fourth. This observation might explain why we require four dimensions to capture a greater proportion of explained variances, as observed in the PCA.


## Eigenvalues and dominant axes analysis

We want to know how many axes we need to consider for the next Hierarchical Classification stage. We can do this according to the generalized Kaiser theorem: all those dimensions such that their
eigenvalue is greater than the mean. 

```{r}
mean(res.mca$eig[,1])

head(get_eigenvalue(res.mca), 10)

fviz_screeplot(res.mca, addlabels=TRUE, ylim=c(0,20), barfill='darkslateblue',barcolor='darkslateblue', linecolor="skyblue1")
```
As we can see, the eigenvalue mean is 0.1428571. Therefore, we will
take up to dimension 5, which represents the 55.51% of the sample.


## Individuals point of view

We now want to see if there are individuals "too contributive".

```{r}
fviz_mca_ind(res.mca, geom=c('point'),col.ind="contrib", gradient.cols =
c("darkslateblue", "red"))
```

We observe that only three individuals appear to make significant contributions, with a small number showing lesser levels of contribution. However, the vast majority contribute minimally.

```{r}
fviz_mca_ind(res.mca, label="none", habillage= df$sex,
palette=c("darkslateblue", "red"))
```
We see that there are quite a lot more males than females. With females more representend in the second dimension and males in the first.

## Interpreting map of categories


In the subsequent plot, it's evident that quantitative variables display a predominant correlation along the first dimension, characterized by low values. Similarly, the qualitative variables sex and restecg demonstrate comparable correlations along this dimension. Conversely, the remaining qualitative variables showcase stronger correlations with either dimension 1 or 2. Notably, variables thal and slope exhibit similar correlations across both dimensions, whereas variables fbs, cp, and exang show stronger correlations with one dimension over the other—specifically, the second dimension for fbs and the first dimension for the latter two.

```{r}
fviz_mca_var(res.mca, choice="mca.cor", repel=TRUE)
```


Now, we delve into the specific values of the variables. On the left side, we encounter values associated with a lower percentage of heart attacks, predominantly featuring female individuals. Conversely, in the upper right corner, we find values associated with a higher percentage of heart attacks. 

As evident, the values corresponding to a moderate likelihood of experiencing a heart attack are centrally located in the gravity center, predominantly featuring male individuals.

```{r}
fviz_mca_var(res.mca, repel=TRUE)
```

## Interpreting the axes association to factor map


### Variables point of view coordinates

First, we examine the contribution of each category value to each dimension, noting that higher values correspond to greater contributions. So for example, the value female for sex category has a high contribution to dimension 3, while male has a small contributioon to dimensions one and two.

```{r}
res.mca$var$coord
```

### Quality of representation


Now, we assess the quality of representation for each category value by observing the graphical depiction of their representation levels. Notably, the category value "Typical Angina" emerges as the most well-represented in the first two dimensions, while in the third and fourth dimensions, "female" ranks as the second best representation. This aligns with the numerical values obtained earlier.

```{r}
res.mca$var$cos2
fviz_cos2(res.mca, choice = "var", axes = 1:2)
fviz_cos2(res.mca, choice = "var", axes = 3:4)
```

### Contribution of the variables


Here, we observe the graphical representation of contributions for the variable categories. Once more, higher values indicate greater contributions. As expected, the variables contributing the most align with those that were best represented earlier.

```{r}
res.pca$var$contrib
fviz_contrib(res.mca, choice = "var", axes = 1:2)
fviz_contrib(res.mca, choice = "var", axes = 3:4)
```

## MCA with suplementary variables

We now perform the MCA again, but putting as suplemenatry the individuals found as multivariate outliers.

```{r}
res.mca <- MCA(df[,c(vars_res, vars_dis, vars_con)], quanti.sup = c(1,9:15), ind.sup = ll, graph = FALSE)
```

# Hierarchical Clustering (from MCA)
```{r}
res.hcpc.mca <- HCPC(res.mca, nb.clust=-1)
```


The hierarchical clustering using the mca results ends in three different clusters, one less than using pca results.

Test of the association of categorical variables

```{r}
res.hcpc.mca$desc.var$test.chi2  
```

Variables like "slope," "thal," and "exang" show extremely low p-values, suggesting strong relationships.
However, "sex" and "fbs" also exhibit significance, with slightly higher p-values.

```{r}
res.hcpc.mca$desc.var$category[1]
```
This first cluster has a strong influence with variables like exang=No, slope=Downsloping or ca=Non anginal pain, some less characteristic variables could be slope=Flat, cp=Typical Angina and exang=Yes, suggesting individuals with this characteristics are not typical of this cluster.

```{r}
res.hcpc.mca$desc.var$category[2]
```
Contrary to the first cluster, indivuduals with slope=Flat, cp=Typical Angina and exang=Yes are common in this second cluster, other non typical characteristics of this group could be cp=Non-anginal Pain, slope=Downsloping and exang=No.

```{r}
res.hcpc.mca$desc.var$category[3]
```
Finally the third cluster is composed with individuals with thal=Fixed Defect, slope=Upsloping and restecg=Hypertrophy. On the other hand, fbs =< 120 mg/dL, slopw=Downsloping and thal=Reversible Defect are some of the less common characteristics in individuals in this third cluster.

```{r}
res.hcpc.mca$desc.var$quanti.var
```
Variables like "oldpeak," "thalach," and "ca" exhibit notably high Eta2 values and extremely low p-values, indicating substantial contributions and strong associations.
On the other hand, "age," "trestbps," and "chol" also show statistical significance but with relatively lower Eta2 values, suggesting comparatively lesser impact on the dataset's variability.

```{r}
res.hcpc.mca$desc.var$quanti[1]
```
We can see individuals with a high maximum heart rate (thalach) in this cluster, with a little below the average cholesterol and trestbps, and with a low number of major vessels detected by fluoroscopy (ca) and a low ST depression (oldpeak).


```{r}
res.hcpc.mca$desc.var$quanti[2]
```
In this second cluster we can observe a high influence in individuals with a high ST depression, a high numer of vessels in the fluoroscopy test and generally above the average in terms of age. These individuals have also a high cholesterol comparing it to the mean.

```{r}
res.hcpc.mca$desc.var$quanti[3]
```
Finally in this last group we can find extremely high ST depression and a high blood pressure. The typical individuals of this clusters have an above the mean age, and they tend to have a low max heart rate.


## Parangons
```{r}
res.hcpc.mca$desc.ind$para
```
This are the Parangons for cluster 1, 2 and 3.

## Class-specific variables

```{r}
res.hcpc.mca$desc.ind$dist
```

Class-specific individuals for clusters 1, 2 and 3.


## Comparison of clusters obtained after K-Means (based on PCA) and/or Hierarchical Clustering (based on PCA) focusing on the target.

thalach   15.567596      159.6424581  149.1015779     17.9693175 22.8658841
target.1  14.574760        0.6609870    0.5376923      0.2495858  0.2856772
target    14.574760        0.6609870    0.5376923      0.2495858  0.2856772
chol      -2.253698      242.9888268  246.4319527     51.8568064 51.5928799
...

In the first cluster of hierarchical clustering using mca we can find a high influence of individuals with a high probability of getting a heart attack, the cluster mean is significantly higher than the global mean. We can see is also related to individuals with a high maximum heart rate (thalach), with a little below the average cholesterol and trestbps, and with a low number of major vessels detected by fluoroscopy (ca) and a low ST depression (oldpeak).

thalach   17.165341      165.7713499  149.1141463     14.0882717 22.9944987
target.1  12.802854        0.6907438    0.5363902      0.2309836  0.2856828
target    12.802854        0.6907438    0.5363902      0.2309836  0.2856828
trestbps  -9.149772      124.8512397  131.6117073     12.1240111 17.5081712
chol      -9.628293      225.0468320  246.0000000     36.7901578 51.5673370
oldpeak  -12.064233        0.4735537    1.0715122      0.7936492  1.1744799
ca       -13.476351        0.1542700    0.6878049      0.4307751  0.9381339
age      -21.838374       46.0771350   54.4341463      6.5706251  9.0678636

A similar conclusion can be done by looking at the first cluster of the herarchical clustering using PCA, seeing the same individuals with a high heart rate, low cholesterol, young...

thalach   10.046405      159.9523810  149.1141463     14.4981976 22.9944987
target     7.550224        0.6375873    0.5363902      0.2623760  0.2856828
chol       3.636029      254.7968254  246.0000000     43.6305193 51.5673370
trestbps   2.602203      133.7492063  131.6117073     11.5521162 17.5081712
age       -2.229345       53.4857143   54.4341463      6.2168960  9.0678636
ca        -7.620883        0.3523810    0.6878049      0.6470586  0.9381339
oldpeak  -10.487070        0.4936508    1.0715122      0.6486795  1.1744799

Finally all said before resonates with the first cluster of kmeans, all variables have a similar means and go on the same line.














# Linear model building


```{r}
df.original <- df[, 1:14]

# Predict duration with the variable age
m1 <- lm(formula = target ~ age, data=df.original)

summary(m1)
```
The low R-squared value (0.04662) tells us that the model does not explain much of the variability in the target variable. This suggests that age alone is not a strong predictor. Given this, the predictive power of age on the target variable is considered “weak.”

However, knowing that our data might have particular characteristics, we should consider further analysis. Specifically, we should examine if the relationship between age and the target variable is different for individuals over 45 years old. This subgroup analysis might reveal patterns or relationships that are not apparent when considering the entire dataset.

```{r}

df2.original <- df2[,1:14] #(df2 té age > 45)


# Predict duration with the variable age
m2 <- lm(formula = target ~ age, data=df2.original)

summary(m2)
```
We conducted a new linear regression analysis using only the subset of data where the age is greater than 45 (df2). The aim was to see if the relationship between age and the target variable improves in this specific age group. However, the results indicate that there is still not a significant improvement. 

The results show that limiting the dataset to individuals older than 45 did not improve the predictive power of age on the target variable. The R-squared value is still very low, and the age coefficient is not significant.


```{r}

mNumAll <- lm(formula = target ~ age + trestbps + chol + thalach + oldpeak + ca, data=df.original)

summary(mNumAll)

```

The inclusion of multiple predictors has significantly improved the model compared to using age alone. The R-squared value has increased from 0.04662 (when using age alone) to 0.3366, indicating a much better fit. This means that the combined effect of these variables explains about 33.66% of the variability in the target variable, compared to only 4.662% when using age alone.

Although the R-squared value is still relatively low, the model with multiple numeric predictors provides a much better fit than using age alone. This improvement demonstrates the value of incorporating multiple relevant predictors to enhance the explanatory power of the regression model.

```{r}

mNum <- lm(formula = target ~ age + trestbps + chol + thalach + oldpeak + ca, data=df2.original)

summary(mNum)

```

When we use only numeric variables from the subset of data where age is greater than 45, the R-squared value is 0.3317. This suggests that these predictors explain about 33.17% of the variability in the target variable.

Given that the R-squared values are still relatively low, it indicates that the models, even with multiple predictors, do not capture all the factors affecting the target variable. Therefore, the next step is to explore the impact of the factors directly.



Let's try Stepwise method in both cases:


```{r}
# Using stepwise methodology to get the most optimize linear regression model with Akaike criteria
m3 <- lm(formula= target ~ ., data =df.original)

m_step = step(m3)
summary(m_step)
```

The stepwise regression method has helped us identify a more optimized model with a higher R-squared value, indicating a better fit to the data, this the one with all the variables. However, we notice that some predictors are not statistically significant.



```{r}
# Using stepwise methodology to get the most optimize linear regression model with Akaike criteria
m4 <- lm(formula= target ~ ., data =df2.original)

m_step = step(m4)
summary(m_step)
```
Improvement in R-squared:
- The optimized model has a high R-squared value of 0.6637, indicating a strong fit. This means the predictors explain a significant portion of the variability in the target variable.

Significant Predictors:
- Variables such as sex, cp (Typical Angina), trestbps, exang, slope (Flat), and ca are significant predictors, indicating their strong relationship with the target variable.
- Some predictors like age are marginally significant, suggesting a potential impact.

Non-significant Predictors:
- Variables such as cp (Atypical Angina), cp (Non-anginal Pain), and slope (Upsloping) have high p-values and are not statistically significant. These categories could potentially be removed to simplify the model further without losing much predictive power.

Also we got some observations to mention:
- Some coefficients have signs contrary to logical expectations. For example, trestbps is positive, suggesting higher blood pressure is associated with a healthier outcome, which contradicts typical medical understanding. This anomaly might require further investigation.
- The ca variable has a negative coefficient, which is not expected, as the number of major vessels colored by fluoroscopy should logically correlate positively with health.
- Certain categories like exang (Yes) and cp (Typical Angina) have coefficients and significance levels that align with medical understanding, indicating their importance in predicting the target variable.




Now we will try to optimize all the categories with a p-value > 0.05, in order to see if we keep them or not.

```{r}
optimize = lm(formula = target ~ sex + cp + trestbps + chol + thalach + 
    exang + oldpeak + slope + ca, data = df.original)
summary(optimize)
```

```{r}
plot(optimize)
```

1. Residuals vs Fitted Plot
- Pattern Observed: The plot shows two distinct bands of residuals, one at lower fitted values and one at higher fitted values. There is a noticeable gap between 0.3 and 0.5 in the fitted values.
- Interpretation: This pattern suggests heteroscedasticity, where the variance of the residuals changes with the fitted values. The gap likely indicates that the target variable has no values in that range, leading to a jump in the residuals.

2. Normal Q-Q Plot
- Pattern Observed: The residuals mostly follow the reference line, but there are deviations at both ends.
- Interpretation: The residuals are approximately normally distributed, but there are some outliers. The deviations at the tails suggest potential issues with normality in the extremes.

3. Scale-Location Plot
- Pattern Observed: The spread of the residuals seems to increase with the fitted values, forming a funnel shape.
- Interpretation: This pattern further confirms heteroscedasticity, where the variance of residuals is not constant. It suggests that the model might be less reliable for higher fitted values.

4. Residuals vs Leverage Plot
- Pattern Observed: Most residuals are centered around zero with a few points having high leverage and high standardized 
- Interpretation: This plot helps identify influential points that have a significant impact on the model’s fit. Points outside the Cook’s distance lines are potentially influential observations that may need further investigation.


From those graphics we can get this conclusions:
- Gap in Fitted Values: The gap in the first plot suggests that the target variable values are not continuous across the range but jump from 0.4 to 0.6, skipping intermediate values. This may indicate an issue with the distribution of the target variable.
- Heteroscedasticity: Both the Residuals vs Fitted and Scale-Location plots indicate heteroscedasticity. This violates one of the key assumptions of linear regression, which assumes constant variance of residuals.
- Outliers and Influential Points: The Normal Q-Q plot shows some deviations at the tails, indicating outliers. The Residuals vs Leverage plot shows some points with high leverage, which could disproportionately affect the model.



```{r}
optimize1.1 = lm(formula = target ~ sex + trestbps + chol + thalach + 
    exang + oldpeak + slope + ca, data = df.original)
summary(optimize1.1)
```

Here we remove we have removed cp. The model has a lower value of 0.4453. Predictors have a similar significance when compared to the previous iteration, both strong predictors and weak predictors. So the result is a little worse.

```{r}
optimize1.2 = lm(formula = target ~ sex + trestbps + chol + thalach + 
    exang + oldpeak + ca, data = df.original)
summary(optimize1.2)
```

Similarly removing slope gives also a slightly worse result, with similar significance of the predictors.

```{r}
optimize1.3 = lm(formula = target ~ sex + trestbps + chol + thalach + 
    exang + oldpeak + cp + ca, data = df.original)
summary(optimize1.3)
```

We try a last attempt without slope but adding cp. The result has improved but it keeps much lower than our model with all the variables.

Now it would be interesting to try if our less biased data frame with the individuals with more than 45 years performs better:

```{r}
optimize2 = lm(formula = target ~ age + sex + cp + trestbps + exang + slope + 
    ca, data = df2.original)
summary(optimize2)
```

```{r}
plot(optimize2)
```

This first model with some of the variables gives as already a similar result in terms of R-squared to our complete data frame with all the variables. 
We will now perform some other tests to see if we can push it a little further.

Both the Residuals vs Fitted and Scale-Location plots keep indicating heteroscedasticity. Even with this second data frame.

```{r}
optimize2.1 = lm(formula = target ~  sex + cp + trestbps + exang + slope + 
    ca, data = df2.original)
summary(optimize2.1)
```

We have also tried in this new model to exclude some of the less significant variables. Here we can see by removing age that it hasn't much effect but it's a little bit worse.

```{r}
optimize2.1 = lm(formula = target ~  age + sex + trestbps + exang + slope + 
    ca, data = df2.original)
summary(optimize2.1)
```


```{r}
optimize2.1 = lm(formula = target ~ sex + trestbps + exang + 
    ca, data = df2.original)
summary(optimize2.1)
```

After all this test we see that the best model is the one obtained with all the variables and optimize, and also that we have better results using df2. By looking at the graphics of df2 we cannot see as many patters, but this is also because it has less individuals.

Finally we introduce a new column indicating the multivariate outliers and we see if they have significance in the model:

```{r}
mMoutAll <- lm(formula= target ~ ., data =df[,1:15])
summary(mMoutAll)
```

```{r}
par(mfrow = c(2, 2))
plot(mMoutAll)
```

```{r}
mMout <- lm(formula= target ~ ., data =df2[,1:15])
summary(mMout)
```
```{r}
par(mfrow = c(2, 2))
plot(mMout)
```

In both the full data frames, df and df2 we can see some improvement on the model by using the variable mOut. This last one is our model with better results.

### Target variable transformation?

```{r}
library(MASS)
boxcox(optimize,data=df.original)
```


Lambda is lower than one and higher than 0, but being really close to 1 makes us consider trying the square root transformation to the target variable. 

```{r}
transm <- lm(formula= sqrt(target) ~ ., data =df[,1:15])
summary(transm)
```

```{r}
par(mfrow = c(2, 2))
plot(transm)
```

```{r}
transm45 <- lm(formula= sqrt(target) ~ ., data =df2[,1:15])
summary(transm45)
```

```{r}
par(mfrow = c(2, 2))
plot(transm45)
```

Both cases where we apply the transformation are worse, so we confirm that the value lambda being next to 1 makes the transformation not necessary.

# Binary Regression Model

We begin by creating two new datasets: one from the original dataset and another subset where the age is greater than 45. In these datasets, we transform the numerical target variable into a binary variable. Specifically, we assign a value of 1 to the target variable if it is greater than or equal to 0.5, and a value of 0 otherwise.

```{r}
dfb <- df.original[,c(1:14)]
dfb$target[which(dfb$target>=0.5)] <- 1
dfb$target[which(dfb$target<0.5)] <- 0

head(dfb$target, 100)
```


```{r}
dfb45 <- df2[,c(1:14)]
dfb45$target[which(dfb45$target>=0.5)] <- 1
dfb45$target[which(dfb45$target<0.5)] <- 0

head(dfb45$target, 100)
```

We then proceed to split the datasets into training and testing, with 80-20 split respectively. 

```{r}
library(dplyr)

# Divide the data set in training and test split (80-20 split)
set.seed(123)  # Ensure the results are repeatable
sample_size <- floor(0.8 * nrow(dfb))
train_index <- sample(seq_len(nrow(dfb)), size = sample_size)
train_data <- dfb[train_index, ]
test_data <- dfb[-train_index, ]
```


```{r}
library(dplyr)

# Divide the data set in training and test split (80-20 split)
set.seed(123)  # Ensure the results are repeatable
sample_size45 <- floor(0.8 * nrow(dfb45))
train_index45 <- sample(seq_len(nrow(dfb45)), size = sample_size45)
train_data45 <- dfb45[train_index45, ]
test_data45 <- dfb45[-train_index45, ]
```

In this analysis, we begin by building a generalized linear model (GLM) using logistic regression to predict the binary target variable. We use all available predictors in the training dataset. The binomial family is chosen to perform logistic regression, suitable for binary classification tasks.

After fitting the model (mB), we generate a detailed summary to examine the model's coefficients, their statistical significance, and overall fit. This summary provides insights into how each predictor influences the target variable and helps assess the model's performance through various diagnostic metrics.

We then calculate the Variance Inflation Factor (VIF) for each predictor to check for multicollinearity. High VIF values indicate that some predictors are highly correlated with each other, which can affect the model's stability and interpretation. Identifying and addressing high VIF values is crucial for ensuring a robust and reliable model.

```{r}
mB <- glm(formula= target ~ ., data = train_data, family=binomial)
summary(mB)
vif(mB)
```
Several predictors are highly significant (e.g., sexMale, cpAtypical Angina, trestbps, chol, exangYes, slopeFlat, ca), suggesting they have a strong influence on the target variable.

All VIF values are below 2, indicating that multicollinearity is not a significant concern in this model.

```{r}
mB45 <- glm(formula= target ~ ., data = train_data45, family = binomial)

summary(mB45)
vif(mB45)
```
The model shows an extremely low residual deviance and high standard errors for coefficients, suggesting that the model is overfitting the training data.
The high number of Fisher Scoring iterations (25) indicates potential issues with model convergence.

None of the predictors are statistically significant, as indicated by their p-values close to 1.

High VIF values for several predictors indicate severe multicollinearity, which can cause instability in the coefficient estimates and make it difficult to determine the individual effect of each predictor.

For these reasons, we'll continue with only the original dataset and its training and test data.

As our initial model, we selected an additive model that includes the variables ca, oldpeak, and thalach, based on their strong correlation with the target variable, as seen in the correlation plot of the first deliverable.

```{r}
initial_model <- glm(target ~ ca + oldpeak + thalach, data = train_data, family = binomial)
```

We then try to build some other models, considering the insights gleaned from summarizing the findings of model mB, which highlighted the most significant variables. Initially, we focus on the significant numerical variables and gradually augment the model complexity by incorporating additional influential factors. Subsequently, we conduct an ANOVA test to determine the superior model among them.

```{r}
initial_model2 <- glm(target ~ trestbps + ca, data = train_data, family = binomial)

initial_model2.2 <- glm(target ~ ca + oldpeak + thalach + sex + exang , data = train_data, family = binomial)

initial_model3 <- glm(target ~ trestbps + ca + cp + sex + exang + slope, data = train_data, family = binomial)

initial_model3.2 <- glm(target ~ oldpeak + thalach + ca + cp + sex + exang + slope, data = train_data, family = binomial)

initial_model4 <- glm(target ~ cp + trestbps + slope + ca + sex + exang + chol + thalach + oldpeak, data = train_data, family = binomial)


anova(initial_model, initial_model2)
anova(initial_model, initial_model2.2)
anova(initial_model2.2, initial_model3)
anova(initial_model3.2, initial_model3)
anova(initial_model3.2, initial_model4)
```

In each ANOVA test, we select the model with the lowest residual deviance to proceed to the next stage. Initial_model4 seems to be the best model. However, despite its superior performance, it demands a larger set of predictors compared to other models, and its improvement in residual deviance isn't substantial. Therefore, we opt to retain initial_model3, which strikes a favorable balance between the number of predictors and the reduction in residual deviance.



We use stepwise methodology to get the most optimize linear regression model with Akaike criteria.

```{r}
model <- glm(target ~ .,  data = train_data, family=binomial)

m_step_mB = step(model)
anova(m_step_mB, initial_model3)

summary(m_step_mB)
```

The top-performing model identified through stepwise selection employs twice the number of predictors compared to our chosen initial_model3. However, upon examining its summary, we observe that the variables selected in initial_model3 remain the most significant, denoted by their significance level (***). Despite the doubling of predictors, these variables maintain their importance. Therefore, to enhance the model further while avoiding excessive predictor inclusion, we aim to refine initial_model3 by exploring potential interactions between variables.


```{r}
final_model <- initial_model3

vif(final_model)
```
Once again, we see low VIF values which confirm that multicollinearity is not a significant issue, ensuring the stability and reliability of the final_model coefficients.

We now proceed to consider interactions between factors and between factors and covariate. 

```{r}
# Initial model without interaction
model_no_interaction <- glm(formula = target ~ sex + exang + cp + trestbps + slope + ca, family = binomial, data = train_data)

# Model with interaction
model_with_interaction <- glm(formula = target ~ sex * exang + cp + trestbps + slope + ca, family = binomial, data = train_data)

# Compare models using ANOVA
anova(model_no_interaction, model_with_interaction)
```
We see a slight improvement when considering the interaction between factors sex and exang.

```{r}
# Initial model without interaction
model_no_interaction2 <- glm(formula = target ~ sex + cp + exang + trestbps + slope + ca, family = binomial, data = train_data)

# Model with interaction
model_with_interaction2 <- glm(formula = target ~ sex * cp + exang + trestbps + slope + ca, family = binomial, data = train_data)

# Compare models using ANOVA
anova(model_no_interaction2, model_with_interaction2)
```
The enhancement observed by incorporating the interaction between the factors sex and cp is minimal.

```{r}
# Initial model without interaction
model_no_interaction3 <- glm(formula = target ~ sex + slope + cp + trestbps + exang + ca, family = binomial, data = train_data)

# Model with interaction
model_with_interaction3 <- glm(formula = target ~ sex * slope + cp + trestbps + exang + ca, family = binomial, data = train_data)

# Compare models using ANOVA
anova(model_no_interaction3, model_with_interaction3)
```
We see the same minimal improvement as in the previous example.

```{r}
# Initial model without interaction
model_no_interaction_covariate <- glm(formula = target ~ sex + ca + exang + trestbps + slope + cp, family = binomial, data = train_data)

# Model with interaction
model_with_interaction_covariate <- glm(formula = target ~ sex * ca + exang + trestbps + slope + cp, family = binomial, data = train_data)

# Compare models using ANOVA
anova(model_no_interaction_covariate, model_with_interaction_covariate)
```
We see even less minimal improvement than in the two previous example.

```{r}
# Initial model without interaction
model_no_interaction_covariate2 <- glm(formula = target ~ sex + trestbps + exang + ca + slope + cp, family = binomial, data = train_data)

# Model with interaction
model_with_interaction_covariate2 <- glm(formula = target ~ sex * trestbps + exang + ca + slope + cp, family = binomial, data = train_data)

# Compare models using ANOVA
anova(model_no_interaction_covariate2, model_with_interaction_covariate2)
```
We see the same minimal improvement as in the previous example.

```{r}
# Initial model without interaction
model_no_interaction_covariate3 <- glm(formula = target ~ exang + trestbps + sex + ca + slope + cp, family = binomial, data = train_data)

# Model with interaction
model_with_interaction_covariate3 <- glm(formula = target ~ exang * trestbps + sex + ca + slope + cp, family = binomial, data = train_data)

# Compare models using ANOVA
anova(model_no_interaction_covariate3, model_with_interaction_covariate3)
```
We see a slight improvement when considering the interaction between factor exang and covariate trestbps.

```{r}
# Initial model without interaction
model_no_interaction_covariate4 <- glm(formula = target ~ exang + cp + sex + ca + slope + trestbps, family = binomial, data = train_data)

# Model with interaction
model_with_interaction_covariate4 <- glm(formula = target ~ exang * cp + sex + ca + slope + trestbps, family = binomial, data = train_data)

# Compare models using ANOVA
anova(model_no_interaction_covariate4, model_with_interaction_covariate4)
```
We see a slight improvement, better than the previous one, when considering the interaction between factor exang and covariate cp.

After seeing the improvement of single interactions, we want to see if the models improve when having two interactions. 

```{r}
# Model with interaction
model_with_interaction_double <- glm(formula = target ~ exang * (cp + trestbps) + ca + slope + sex, family = binomial, data = train_data)

# Compare models using ANOVA
anova(model_with_interaction_covariate3, model_with_interaction_double)
anova(model_with_interaction_covariate4, model_with_interaction_double)
anova(model_no_interaction, model_with_interaction_double)
```
We see that considering the interactions between factor exang and factors trestbps and cp the model improves much more compared to a single interaction.

We now try different models adding other interactions with exang.

```{r}

# Model with interaction
model_with_interaction_triple <- glm(formula = target ~ exang * (cp + trestbps + sex) + ca + slope, family = binomial, data = train_data)

model_with_interaction_q <- glm(formula = target ~ exang * (cp + trestbps + sex + ca) + slope, family = binomial, data = train_data)

model_with_interaction_c <- glm(formula = target ~ exang * (cp + trestbps + sex + ca + slope), family = binomial, data = train_data)

# Compare models using ANOVA
anova(model_with_interaction_double, model_with_interaction_triple)

anova(model_with_interaction_triple, model_with_interaction_q)
anova(model_with_interaction_q, model_with_interaction_c)
anova(final_model, model_with_interaction_c)
anova(m_step_mB, model_with_interaction_c)
```
We see that model_with_interaction_c improves much, not only with respect to final_model but also to the best found model m_step_mB, which had 10 predictors.

As model_with_interaction_c has 5 interactions, we want to see how this affects multicollinearity.

```{r}
vif(model_with_interaction_c)
```
We see that exang has now a value over 10 caused by the interactions with trestbps and sex. We now proceed to substract this variables and check again for its VIF values.

```{r}
# Model with interaction
model_with_interaction_c2 <- glm(formula = target ~ exang * (cp + ca + slope) + trestbps + sex, family = binomial, data = train_data)

anova(final_model, model_with_interaction_c2)
```
We see that removing the two interactions does not worsen the model much, and still performs better than the final model.


```{r}
vif(model_with_interaction_c2)
```

The VIF values observed are consistently low across all predictors. Although exang shows a slightly higher value of three, this doesn't necessarily imply that multicollinearity is a concern for the model.


We've opted to assess both the final_model and the model_with_interaction_c2. Despite the latter's superior performance, we're conducting this comparison to ensure that introducing interactions doesn't introduce any unforeseen issues.

```{r}
# Diagnostic plots for the final_model and model_with_interaction_c2
par(mfrow = c(2, 2))
plot(final_model)
plot(model_with_interaction_c2)
```
Residuals vs Fitted:
The residuals display a funnel shape, indicating heteroscedasticity. This means the variance of the residuals is not constant across levels of the fitted values, violating the homoscedasticity assumption.

Q-Q Plot:
The residuals deviate from the 45-degree line, particularly in the tails, suggesting that they do not follow a normal distribution. This is an indication that the normality assumption of the residuals is violated.

Scale-Location:
The spread of the residuals increases with the predicted values, reinforcing the presence of heteroscedasticity. The residuals are not evenly spread, indicating issues with the model's assumptions.

Residuals vs. Leverage Plot:
This plot shows the relationship between standardized residuals and leverage. Most data points are clustered with low leverage and residuals near zero. A few points have higher leverage or residuals, indicating potential influential observations, but no points exceed Cook's distance threshold significantly.

For both models we see more or less the same results, except with Residuals vs. Leverage Plot where there seems to be some influential point for model_with_interaction_c2, which we will look into next.

```{r}
# Identify specific influential observations
influential_points <- which(influence.measures(model_with_interaction_c2)$is.inf)

# Highlight influential points in plots
plot(model_with_interaction_c2, which = 4, id.n = length(influential_points))
```
Cook's Distance Plot:
This plot identifies influential data points in the model. Observations such as 39, 643, and 475 have relatively higher Cook's distances, suggesting these points have a greater influence on the model's coefficients. Overall, most points have low Cook's distances, indicating minimal individual influence.

```{r}
# Avaluació del model amb les dades de test
predictions <- predict(final_model, newdata = test_data, type = "response")
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
conf_matrix <- table(Predicted = predicted_classes, Actual = test_data$target)

# Calcular mètriques d'avaluació (Accuracy, Sensitivity, Specificity)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
sensitivity <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
specificity <- conf_matrix[1, 1] / sum(conf_matrix[1, ])

# Mostrar la matriu de confusió i les mètriques d'avaluació
print(conf_matrix)
cat("Accuracy: ", accuracy, "\n")
cat("Sensitivity: ", sensitivity, "\n")
cat("Specificity: ", specificity, "\n")
```
The confusion matrix presents the model's performance.
Accuracy: 86.83%
Sensitivity (True Positive Rate): 86.09%
Specificity (True Negative Rate): 87.78%
The model shows balanced performance in predicting both classes with high accuracy, sensitivity, and specificity.
```{r}
# Avaluació del model amb les dades de test
predictions <- predict(model_with_interaction_c2, newdata = test_data, type = "response")
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
conf_matrix <- table(Predicted = predicted_classes, Actual = test_data$target)

# Calcular mètriques d'avaluació (Accuracy, Sensitivity, Specificity)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
sensitivity <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
specificity <- conf_matrix[1, 1] / sum(conf_matrix[1, ])

# Mostrar la matriu de confusió i les mètriques d'avaluació
print(conf_matrix)
cat("Accuracy: ", accuracy, "\n")
cat("Sensitivity: ", sensitivity, "\n")
cat("Specificity: ", specificity, "\n")
```
The confusion matrix presents the model's performance.
Accuracy: 87.80%
Sensitivity (True Positive Rate): 86.32%
Specificity (True Negative Rate): 89.77%
The model shows balanced performance in predicting both classes with high accuracy, sensitivity, and specificity.

After seeing the results of predictions by the two models, even though  model_with_interaction_c2`s leverage vs residuals plot seemed to have some influential points, three as seen in the influence plot, it does not really affect the model as it still gets slightly  better results than the final_model. But the difference is really small.